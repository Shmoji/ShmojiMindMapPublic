  * questions
    * storage size of files will grow way too large over time, right?
      * David's response: idea is to trade off disk space in exchange for token efficiency.
      * It is not meant to be GB or TB of data, but rather to rely on semantic embeddings and local SSD disk space to very quickly reconstruct CONVERSATIONAL TOPICS for the sake of chatbots
  * TODO eventually (roadmap)
    * fuzzy search and forking to allow broader search
  * issues
    * it seems if you stop program and get back to it later, it cannot recall old convo
  * David anecdotes
    * REMO is a microservice
    * REMO uses CLUSTERING (without it, how can any level of semantic relevance be achieved?) - in reference to Llama Tree Index which doesnt use clustering.
  * experimenting
    * Try1
      * Results
        * I sent about 8 messages to raven and ran the temporal consolidation file. Looks like it cost about 7 cents
        * I was about done with this goal/convo after about 8 messages.
        * what is size of all necessary files generated by this convo?
          * really not bad at all. total of around 180KB
          * layer2 log files take most space so far, maybe due to embeddings?
  * stored files / data
    * layers
      * layer 1 logs save user chats and AI (RAVEN) responses without change
        * generated by chat.py
      * layer 2 temporal logs
        * generated by step01_temporal_consolidation.py, not by chat.py
        * it saves 1 json file (summary) for every 5 chat log files (including both user and Raven files).
        * save many key/values on run of step01_temporal_consolidation.py. If no new layer 1 logs, do not run anything. If there are NEW layer 1 logs (means user chatted with RAVEN) since last run of step01_temporal_consolidation.py, then run program that stores summary file with:
          * all new layer 1 logs analyzed on this run. No old, already analyzed, logs
          * English summary of all those logs combined
          * timestamps for timeEnd and timeStart (time of what??)
          * vector embeddings of English summary from above
    * other
      * gpt3_logs
        * my thoughts
          * this seems pointless bc you already store all chat logs in layer 1 logs, the default prompt template in some file, and summary in layer 2 logs. All these together basically make up these gpt3_logs. Just better for David's debug process maybe?
        * generated by step01_temporal_consolidation.py, not by chat.py
        * stores the prompt and GPT response that is the summary for all NEW chat logs.