  * what is it?
    * component used in natural language processing (NLP) that breaks down text into smaller units called tokens.
    * HELPs LLMs understand input better (DATA PROCESSING before encoding): Tokenizers help in standardizing and preparing text data for further analysis or processing by converting it into a format that can be easily understood by machine learning models.
  * key points
    * typically happens to input from user before encoding