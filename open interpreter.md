  * tracking thoughts
    * [[2024-03-28]]
      * i mean this looks amazing, but i spent 2 days and not too great
      * i was using ollama to use open interpreter locally
      * i used llama7b model and it was quick speed but it just couldnt do any task at all
      * so i tried using the model used in example on OI repo - mixtral 8x7B model or something like that. it ran so slowly that it was unusable. i assume bc it is big model or something. noticed GPU was not running as much as id expect so thought maybe GPU just wasnt being used to run it - otherwise it would work fast. no docs on how to fix this
      * gave up
